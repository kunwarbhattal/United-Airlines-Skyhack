{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f993b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EDA Q1: Departure Delay Summary\n",
      "| Metric                             | Value   | Unit/Context              |\n",
      "|:-----------------------------------|:--------|:--------------------------|\n",
      "| Average Departure Delay            | 21.18   | Minutes                   |\n",
      "| Flights Departing Late (Delay > 0) | 49.61%  | Percentage of All Flights |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    df_flight = pd.read_csv(\"Flight Level Data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Flight Level Data.csv' not found. Please check your file paths.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df_flight['scheduled_departure_datetime_local'] = pd.to_datetime(df_flight['scheduled_departure_datetime_local'], utc=True)\n",
    "df_flight['actual_departure_datetime_local'] = pd.to_datetime(df_flight['actual_departure_datetime_local'], utc=True)\n",
    "\n",
    "df_flight['departure_delay_minutes'] = (\n",
    "    df_flight['actual_departure_datetime_local'] - df_flight['scheduled_departure_datetime_local']\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "total_flights = df_flight.shape[0]\n",
    "\n",
    "average_delay = df_flight['departure_delay_minutes'].mean()\n",
    "\n",
    "delayed_flights_count = df_flight[df_flight['departure_delay_minutes'] > 0].shape[0]\n",
    "percentage_delayed = (delayed_flights_count / total_flights) * 100\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Average Departure Delay',\n",
    "        'Flights Departing Late (Delay > 0)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{average_delay:.2f}\",\n",
    "        f\"{percentage_delayed:.2f}%\"\n",
    "    ],\n",
    "    'Unit/Context': [\n",
    "        'Minutes',\n",
    "        'Percentage of All Flights'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_output_q1 = pd.DataFrame(data)\n",
    "\n",
    "print(\"### EDA Q1: Departure Delay Summary\")\n",
    "print(df_output_q1.to_markdown(index=False))\n",
    "\n",
    "df_flight[['company_id', 'flight_number', 'scheduled_departure_date_local', 'departure_delay_minutes', 'scheduled_ground_time_minutes', 'minimum_turn_minutes']].to_csv('temp_df_flight_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a1f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EDA Q2: Ground Time Tightness Summary\n",
      "| Metric                                                 | Value   |   Count |\n",
      "|:-------------------------------------------------------|:--------|--------:|\n",
      "| Flights with Scheduled Ground Time  Minimum Turn Mins  | 8.05%   |     652 |\n",
      "| Flights with Scheduled Ground Time (Min Turn + 5 Mins) | 9.63%   |     780 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the flight metrics calculated in the previous step\n",
    "try:\n",
    "    df_metrics = pd.read_csv('temp_df_flight_metrics.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'temp_df_flight_metrics.csv' not found. Please run Code Block 1 first.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df_metrics['ground_time_buffer'] = df_metrics['scheduled_ground_time_minutes'] - df_metrics['minimum_turn_minutes']\n",
    "\n",
    "total_flights = df_metrics.shape[0]\n",
    "\n",
    "tight_ground_time_count = df_metrics[df_metrics['ground_time_buffer'] <= 0].shape[0]\n",
    "percentage_tight_ground_time = (tight_ground_time_count / total_flights) * 100\n",
    "\n",
    "very_tight_ground_time_count = df_metrics[df_metrics['ground_time_buffer'] <= 5].shape[0]\n",
    "percentage_very_tight_ground_time = (very_tight_ground_time_count / total_flights) * 100\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Flights with Scheduled Ground Time  Minimum Turn Mins',\n",
    "        'Flights with Scheduled Ground Time (Min Turn + 5 Mins)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{percentage_tight_ground_time:.2f}%\",\n",
    "        f\"{percentage_very_tight_ground_time:.2f}%\"\n",
    "    ],\n",
    "    'Count': [\n",
    "        tight_ground_time_count,\n",
    "        very_tight_ground_time_count\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_output_q2 = pd.DataFrame(data)\n",
    "\n",
    "print(\"### EDA Q2: Ground Time Tightness Summary\")\n",
    "print(df_output_q2.to_markdown(index=False))\n",
    "\n",
    "# Note: No new metrics are generated that need to be saved for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e0e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EDA Q3: Baggage Complexity Summary\n",
      "| Metric                                         |   Value | Unit/Context                        |\n",
      "|:-----------------------------------------------|--------:|:------------------------------------|\n",
      "| Average Ratio of Transfer Bags vs. Origin Bags |  3.7458 | Ratio (Transfer Bags : Origin Bags) |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Bag Level Data\n",
    "try:\n",
    "    df_bag = pd.read_csv(\"Bag+Level+Data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Bag+Level+Data.csv' not found. Please check your file paths.\")\n",
    "    exit()\n",
    "\n",
    "# --- Calculations ---\n",
    "\n",
    "df_bag['transfer_ind'] = df_bag['bag_type'].apply(\n",
    "    lambda x: 1 if x in ['Transfer', 'Hot Transfer'] else 0\n",
    ")\n",
    "df_bag['origin_ind'] = df_bag['bag_type'].apply(lambda x: 1 if x == 'Origin' else 0)\n",
    "\n",
    "flight_key = ['company_id', 'flight_number', 'scheduled_departure_date_local']\n",
    "df_bag_agg = df_bag.groupby(flight_key).agg(\n",
    "    transfer_bags=('transfer_ind', 'sum'),\n",
    "    origin_bags=('origin_ind', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_bag_agg['transfer_to_origin_ratio'] = df_bag_agg.apply(\n",
    "    lambda row: row['transfer_bags'] / row['origin_bags'] if row['origin_bags'] > 0 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "average_bag_ratio = df_bag_agg['transfer_to_origin_ratio'].mean()\n",
    "\n",
    "# --- Tabular Output ---\n",
    "\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Average Ratio of Transfer Bags vs. Origin Bags'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{average_bag_ratio:.4f}\"\n",
    "    ],\n",
    "    'Unit/Context': [\n",
    "        'Ratio (Transfer Bags : Origin Bags)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_output_q3 = pd.DataFrame(data)\n",
    "\n",
    "print(\"### EDA Q3: Baggage Complexity Summary\")\n",
    "print(df_output_q3.to_markdown(index=False))\n",
    "\n",
    "# Save the bag aggregation data for use in the final difficulty score calculation\n",
    "df_bag_agg.to_csv('temp_df_bag_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd25d6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EDA Q4: Passenger Load and Delay Correlation\n",
      "| Metric                                                    |   Value | Unit/Context   |\n",
      "|:----------------------------------------------------------|--------:|:---------------|\n",
      "| Correlation: Load Factor vs. Departure Delay              | -0.1763 | Pearson R      |\n",
      "| Avg Delay for High Load Flights (Load Factor $\\geq$ 0.99) | 14.48   | Minutes        |\n",
      "| Avg Delay for Low Load Flights (Load Factor $\\leq$ 0.85)  | 37.84   | Minutes        |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    # Load metrics from previous step (Q1)\n",
    "    df_metrics = pd.read_csv('temp_df_flight_metrics.csv')\n",
    "    # Load PNR data for passenger counts\n",
    "    df_pnr_flight = pd.read_csv(\"PNR+Flight+Level+Data.csv\")\n",
    "    # Load Flight Level Data again for total_seats, as it wasn't saved in temp_df_flight_metrics.csv in Q1\n",
    "    # Re-loading df_flight directly is safer here to ensure all columns (like total_seats) are present.\n",
    "    df_flight = pd.read_csv(\"Flight Level Data.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    exit()\n",
    "\n",
    "flight_key_cols = ['company_id', 'flight_number', 'scheduled_departure_date_local']\n",
    "\n",
    "pax_key_cols = flight_key_cols + ['record_locator']\n",
    "df_pax = df_pnr_flight.groupby(pax_key_cols)['total_pax'].max().reset_index()\n",
    "df_total_pax = df_pax.groupby(flight_key_cols)['total_pax'].sum().reset_index(name='total_pax_booked')\n",
    "\n",
    "df_merged_q4 = df_flight.merge(df_total_pax, on=flight_key_cols, how='left')\n",
    "\n",
    "df_merged_q4['scheduled_departure_datetime_local'] = pd.to_datetime(df_merged_q4['scheduled_departure_datetime_local'], utc=True)\n",
    "df_merged_q4['actual_departure_datetime_local'] = pd.to_datetime(df_merged_q4['actual_departure_datetime_local'], utc=True)\n",
    "df_merged_q4['departure_delay_minutes'] = (df_merged_q4['actual_departure_datetime_local'] - df_merged_q4['scheduled_departure_datetime_local']).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "df_merged_q4['total_pax_booked'] = df_merged_q4['total_pax_booked'].fillna(0)\n",
    "df_merged_q4['load_factor'] = (df_merged_q4['total_pax_booked'] / df_merged_q4['total_seats']).clip(upper=1.0).fillna(0)\n",
    "\n",
    "load_delay_correlation = df_merged_q4['load_factor'].corr(df_merged_q4['departure_delay_minutes'])\n",
    "\n",
    "q3_load = df_merged_q4['load_factor'].quantile(0.75)\n",
    "q1_load = df_merged_q4['load_factor'].quantile(0.25)\n",
    "\n",
    "high_load_delay = df_merged_q4[df_merged_q4['load_factor'] >= q3_load]['departure_delay_minutes'].mean()\n",
    "low_load_delay = df_merged_q4[df_merged_q4['load_factor'] <= q1_load]['departure_delay_minutes'].mean()\n",
    "\n",
    "\n",
    "# --- 5. Tabular Output ---\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Correlation: Load Factor vs. Departure Delay',\n",
    "        f'Avg Delay for High Load Flights (Load Factor $\\\\geq$ {q3_load:.2f})',\n",
    "        f'Avg Delay for Low Load Flights (Load Factor $\\\\leq$ {q1_load:.2f})'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{load_delay_correlation:.4f}\",\n",
    "        f\"{high_load_delay:.2f}\",\n",
    "        f\"{low_load_delay:.2f}\"\n",
    "    ],\n",
    "    'Unit/Context': [\n",
    "        'Pearson R',\n",
    "        'Minutes',\n",
    "        'Minutes'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_output_q4 = pd.DataFrame(data)\n",
    "\n",
    "print(\"### EDA Q4: Passenger Load and Delay Correlation\")\n",
    "print(df_output_q4.to_markdown(index=False))\n",
    "\n",
    "# Save the full merged dataframe for the next step (Q5)\n",
    "df_merged_q4.to_csv('temp_df_merged_q4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814b0cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EDA Q5: SSR and Delay Analysis (Controlling for Load)\n",
      "| Metric                                    |   Value | Unit/Context      |\n",
      "|:------------------------------------------|--------:|:------------------|\n",
      "| Avg Delay for High SSR Flights (Mid Load) |   20.63 | Minutes           |\n",
      "| Avg Delay for Low SSR Flights (Mid Load)  |   13.72 | Minutes           |\n",
      "| SSR Q1 for Low SSR Classification         |    1    | Total SSRs/Flight |\n",
      "| SSR Q3 for High SSR Classification        |    3    | Total SSRs/Flight |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    # Load the intermediate file saved from the previous step (Q4)\n",
    "    df_merged_base = pd.read_csv('temp_df_merged_q4.csv')\n",
    "    df_pnr_flight = pd.read_csv(\"PNR+Flight+Level+Data.csv\")\n",
    "    df_pnr_remark = pd.read_csv(\"PNR Remark Level Data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files or intermediate dataframe: {e}\")\n",
    "    exit()\n",
    "\n",
    "flight_key_cols = ['company_id', 'flight_number', 'scheduled_departure_date_local']\n",
    "pnr_unique_cols = ['record_locator', 'flight_number', 'pnr_creation_date']\n",
    "\n",
    "pnr_cols_to_select = list(set(pnr_unique_cols + flight_key_cols))\n",
    "\n",
    "df_ssr_temp = df_pnr_remark.merge(\n",
    "    df_pnr_flight[pnr_cols_to_select].drop_duplicates(),\n",
    "    on=pnr_unique_cols,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "df_ssr_counts = df_ssr_temp.groupby(flight_key_cols).size().reset_index(name='total_ssr_count')\n",
    "\n",
    "df_final = df_merged_base.merge(df_ssr_counts, on=flight_key_cols, how='left')\n",
    "df_final['total_ssr_count'] = df_final['total_ssr_count'].fillna(0)\n",
    "\n",
    "q3_load = df_final['load_factor'].quantile(0.75)\n",
    "q1_load = df_final['load_factor'].quantile(0.25)\n",
    "\n",
    "q3_ssr = df_final['total_ssr_count'].quantile(0.75)\n",
    "q1_ssr = df_final['total_ssr_count'].quantile(0.25)\n",
    "\n",
    "mid_load_df = df_final[\n",
    "    (df_final['load_factor'] > q1_load) &\n",
    "    (df_final['load_factor'] < q3_load)\n",
    "]\n",
    "\n",
    "high_ssr_delay_mid_load = mid_load_df[mid_load_df['total_ssr_count'] >= q3_ssr]['departure_delay_minutes'].mean()\n",
    "low_ssr_delay_mid_load = mid_load_df[mid_load_df['total_ssr_count'] <= q1_ssr]['departure_delay_minutes'].mean()\n",
    "\n",
    "# --- 4. Tabular Output ---\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Avg Delay for High SSR Flights (Mid Load)',\n",
    "        'Avg Delay for Low SSR Flights (Mid Load)',\n",
    "        'SSR Q1 for Low SSR Classification',\n",
    "        'SSR Q3 for High SSR Classification'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{high_ssr_delay_mid_load:.2f}\",\n",
    "        f\"{low_ssr_delay_mid_load:.2f}\",\n",
    "        f\"{q1_ssr:.1f}\",\n",
    "        f\"{q3_ssr:.1f}\"\n",
    "    ],\n",
    "    'Unit/Context': [\n",
    "        'Minutes',\n",
    "        'Minutes',\n",
    "        'Total SSRs/Flight',\n",
    "        'Total SSRs/Flight'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_output_q5 = pd.DataFrame(data)\n",
    "\n",
    "print(\"### EDA Q5: SSR and Delay Analysis (Controlling for Load)\")\n",
    "print(df_output_q5.to_markdown(index=False))\n",
    "\n",
    "# Save the final merged dataframe for the next deliverable (Difficulty Score)\n",
    "df_final.to_csv('final_merged_analysis_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b636c",
   "metadata": {},
   "source": [
    "## Flight Difficulty Score Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8cda80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Robust Composite Index Score Generation (Final Fix) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_2972\\3161204042.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final_scored = df_features.groupby('scheduled_departure_date_local', group_keys=False).apply(daily_min_max_scale, score_features)\n",
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_2972\\3161204042.py:102: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final_scored = df_final_scored.groupby('scheduled_departure_date_local', group_keys=False).apply(classify_difficulty)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Deliverable 2: Flight Difficulty Score (Composite Index) - Top 10 Ranked Flights\n",
      "| scheduled_departure_date_local   |   flight_number | scheduled_arrival_station_code   |   difficulty_score |   daily_difficulty_rank | difficulty_classification   |\n",
      "|:---------------------------------|----------------:|:---------------------------------|-------------------:|------------------------:|:----------------------------|\n",
      "| 2025-08-01                       |            5376 | LEX                              |               0.67 |                       1 | Difficult                   |\n",
      "| 2025-08-01                       |            1811 | MSP                              |               0.55 |                       2 | Difficult                   |\n",
      "| 2025-08-01                       |            4425 | GRR                              |               0.53 |                       3 | Difficult                   |\n",
      "| 2025-08-01                       |            5603 | FNT                              |               0.51 |                       4 | Difficult                   |\n",
      "| 2025-08-01                       |            5103 | JLN                              |               0.46 |                       5 | Difficult                   |\n",
      "| 2025-08-01                       |            4748 | ATW                              |               0.44 |                       6 | Difficult                   |\n",
      "| 2025-08-01                       |            3603 | CMH                              |               0.44 |                       7 | Difficult                   |\n",
      "| 2025-08-01                       |            4778 | FAR                              |               0.42 |                       8 | Difficult                   |\n",
      "| 2025-08-01                       |             972 | BRU                              |               0.40 |                       9 | Difficult                   |\n",
      "| 2025-08-01                       |             468 | IAH                              |               0.40 |                      10 | Difficult                   |\n",
      "\n",
      "File 'flight_difficulty_score_deliverable.csv' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load All Original Data Files ---\n",
    "print(\"--- Starting Robust Composite Index Score Generation (Final Fix) ---\")\n",
    "try:\n",
    "    df_flight = pd.read_csv(\"Flight Level Data.csv\")\n",
    "    df_pnr_flight = pd.read_csv(\"PNR+Flight+Level+Data.csv\")\n",
    "    df_pnr_remark = pd.read_csv(\"PNR Remark Level Data.csv\")\n",
    "    df_bag = pd.read_csv(\"Bag+Level+Data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: One of the required raw data files is missing ({e}). Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "# Define key column set\n",
    "flight_key_cols = ['company_id', 'flight_number', 'scheduled_departure_date_local']\n",
    "pnr_unique_cols = ['record_locator', 'flight_number', 'pnr_creation_date']\n",
    "\n",
    "# --- 2. Recalculate and Merge All 3 Core Features ---\n",
    "\n",
    "# A. Ground Time Risk Feature (from df_flight)\n",
    "# CORRECTION: Include 'scheduled_arrival_station_code' here for the final output\n",
    "core_cols = flight_key_cols + ['scheduled_arrival_station_code', 'scheduled_ground_time_minutes', 'minimum_turn_minutes', 'total_seats']\n",
    "df_features = df_flight[core_cols].copy()\n",
    "\n",
    "df_features['tight_ground_time'] = np.maximum(0, df_features['minimum_turn_minutes'] - df_features['scheduled_ground_time_minutes'])\n",
    "\n",
    "\n",
    "# B. Baggage Complexity Feature (from df_bag)\n",
    "df_bag['transfer_ind'] = df_bag['bag_type'].apply(lambda x: 1 if x in ['Transfer', 'Hot Transfer'] else 0)\n",
    "df_bag['origin_ind'] = df_bag['bag_type'].apply(lambda x: 1 if x == 'Origin' else 0)\n",
    "df_bag_agg = df_bag.groupby(flight_key_cols).agg(\n",
    "    transfer_bags=('transfer_ind', 'sum'),\n",
    "    origin_bags=('origin_ind', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_features = df_features.merge(df_bag_agg, on=flight_key_cols, how='left')\n",
    "df_features[['transfer_bags', 'origin_bags']] = df_features[['transfer_bags', 'origin_bags']].fillna(0)\n",
    "df_features['total_bags_per_seat'] = (df_features['transfer_bags'] + df_features['origin_bags']) / df_features['total_seats'].replace(0, 1)\n",
    "df_features['total_bags_per_seat'] = df_features['total_bags_per_seat'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "\n",
    "# C. Customer/SSR Complexity Feature (from df_pnr_flight & df_pnr_remark)\n",
    "# Recalculate Total Pax and SSRs (Logic remains the same)\n",
    "pax_key_cols = flight_key_cols + ['record_locator']\n",
    "df_pax = df_pnr_flight.groupby(pax_key_cols)['total_pax'].max().reset_index()\n",
    "df_total_pax = df_pax.groupby(flight_key_cols)['total_pax'].sum().reset_index(name='total_pax_booked')\n",
    "\n",
    "ssr_cols_to_select = list(set(pnr_unique_cols + flight_key_cols))\n",
    "df_ssr_temp = df_pnr_remark.merge(\n",
    "    df_pnr_flight[ssr_cols_to_select].drop_duplicates(),\n",
    "    on=pnr_unique_cols,\n",
    "    how='inner'\n",
    ")\n",
    "df_ssr_counts = df_ssr_temp.groupby(flight_key_cols).size().reset_index(name='total_ssr_count')\n",
    "\n",
    "# Merge Pax and SSR counts\n",
    "df_features = df_features.merge(df_total_pax, on=flight_key_cols, how='left')\n",
    "df_features = df_features.merge(df_ssr_counts, on=flight_key_cols, how='left')\n",
    "\n",
    "df_features['total_pax_booked'] = df_features['total_pax_booked'].fillna(0)\n",
    "df_features['total_ssr_count'] = df_features['total_ssr_count'].fillna(0)\n",
    "df_features['ssr_per_pax'] = df_features['total_ssr_count'] / df_features['total_pax_booked'].replace(0, 1)\n",
    "df_features['ssr_per_pax'] = df_features['ssr_per_pax'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "score_features = ['tight_ground_time', 'total_bags_per_seat', 'ssr_per_pax']\n",
    " \n",
    "\n",
    "# --- 3. Daily Normalization and Score Calculation ---\n",
    "\n",
    "def daily_min_max_scale(df, features):\n",
    "    for col in features:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        df[f'{col}_norm'] = (df[col] - min_val) / (max_val - min_val) if (max_val - min_val) != 0 else 0\n",
    "    return df\n",
    "\n",
    "# Apply daily normalization\n",
    "df_final_scored = df_features.groupby('scheduled_departure_date_local', group_keys=False).apply(daily_min_max_scale, score_features)\n",
    "\n",
    "# Calculate Difficulty Score (Equal Weights)\n",
    "df_final_scored['difficulty_score'] = (\n",
    "    df_final_scored['tight_ground_time_norm'] +\n",
    "    df_final_scored['total_bags_per_seat_norm'] +\n",
    "    df_final_scored['ssr_per_pax_norm']\n",
    ") / len(score_features)\n",
    "\n",
    "\n",
    "# --- 4. Daily Ranking & Classification ---\n",
    "\n",
    "def classify_difficulty(df):\n",
    "    df['daily_difficulty_rank'] = df['difficulty_score'].rank(method='min', ascending=False).astype(int)\n",
    "    q_25 = df['difficulty_score'].quantile(0.25)\n",
    "    q_75 = df['difficulty_score'].quantile(0.75)\n",
    "    def classify(score):\n",
    "        if score >= q_75: return 'Difficult'\n",
    "        elif score > q_25: return 'Medium'\n",
    "        else: return 'Easy'\n",
    "    df['difficulty_classification'] = df['difficulty_score'].apply(classify)\n",
    "    return df\n",
    "\n",
    "df_final_scored = df_final_scored.groupby('scheduled_departure_date_local', group_keys=False).apply(classify_difficulty)\n",
    "\n",
    "df_deliverable = df_final_scored[[\n",
    "    'scheduled_departure_date_local', 'flight_number', 'scheduled_arrival_station_code',\n",
    "    'difficulty_score', 'daily_difficulty_rank', 'difficulty_classification'\n",
    "]].sort_values(by=['scheduled_departure_date_local', 'daily_difficulty_rank'])\n",
    "\n",
    "print(\"### Deliverable 2: Flight Difficulty Score (Composite Index) - Top 10 Ranked Flights\")\n",
    "print(df_deliverable.head(10).to_markdown(index=False, floatfmt=\".2f\"))\n",
    "\n",
    "# Save the final scored dataframe for post-analysis\n",
    "df_final_scored.to_csv(\"flight_difficulty_score_deliverable.csv\", index=False)\n",
    "print(\"\\nFile 'flight_difficulty_score_deliverable.csv' saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
